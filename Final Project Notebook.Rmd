---
title: "Final Project Notebook"
authors: "Brian Bacik, Chiu Feng Yap, Joy Yoo, Yi Yao, Yun Qing"
date: "4/14/2021"
output: html_document
---

### Introduction of the Project
The goal of this project is to predict in-hospital ‘death’ of Subarachnoid Hemorrhage (SAH) patients using all patients’ records in the first 3 days from the start time of the hospitalization. No information after the first 3 days should be used as variables. Inputs of prediction includes patients’ encounter data, medications, and procedures.  A test set will be given but the outcome will not be included in the dataset provided to students. 

#### Outcome
The primary outcome is mortality; defined as in-hospital death or discharge to hospice care. 

#### Task 
Build a machine-learning model to predict mortality in subarachnoid hemorrhage patients. 

###  Importing datasets 
```{r message=FALSE}
### loading packages
library(dplyr)
library(readr)
library(tidyr)
library(lubridate)
library(ggplot2)
library(anytime)
library(compareGroups)
library(tidyverse)
library(janitor)
library(randomForest)
library(gbm)
library(glmnet)
library(smotefamily)
library(caret)
library(AUC)
library(pROC)
library(lda)
library(superml)
library(SuperLearner)
library(install.load)
install_load("knitr")
install_load("skimr")
install_load('e1071')
install_load('ROCR')
install_load('cvTools')
```

```{r message=FALSE}
demo_train <- read_csv("demo_train.csv")
med_train <- read_csv("medication_train.csv")
proc_train <- read_csv("procedure_train.csv")
```

### Data Prepocessing
#### Factor-to-time conversion
```{r}
demo_train <- demo_train %>% 
  mutate(New_admitted_dt_tm = anytime(New_admitted_dt_tm), ## anytime() Parse POSIXct or Date objects from input data
         New_discharge_dt_tm = anytime(New_discharge_dt_tm))

med_train$med_started_dt_tm <- anytime(med_train$med_started_dt_tm)

proc_train$procedure_dt_tm <- anytime(proc_train$procedure_dt_tm)
```


#### Renaming columns 
```{r}
demo_train <- demo_train %>% 
  rename(
    patient_num = patient_sk,
    age = age_in_years,
    admission_date = New_admitted_dt_tm,
    discharge_date = New_discharge_dt_tm
  )

med_train <- med_train %>% 
  rename(
    patient_num = patient_sk,
    medication_date = med_started_dt_tm
  )

proc_train <- proc_train %>% 
  rename(
    patient_num = patient_sk,
    procedure_date = procedure_dt_tm
  )
```

#### To combine all prediction variables into one dataset, we first:

    - add patient's admission date to the med_train & proc_train data, then calculate the day difference (X)
    - remove rows from both dataframes that have more than 3 days
    - convert medication and procedure data from long to wide format. 
    - add outcome variable death to each of the medication and procedure data.
    - apply dimension reduction methods (variable selection, PCA) medication and procedure data (Supervised or unsupervised)?
    - combine all the information into the main demo_train dataset that contains patients' demographics
    - calculate length of stay (LOS) and select only those patients have more than or equal to 3 day of LOS. 

```{r}
med_train <- demo_train %>% 
  select(patient_num, admission_date) %>% 
  right_join(med_train, by = "patient_num")
```

```{r}
proc_train <- demo_train %>% 
  select(patient_num, admission_date) %>% 
  right_join(proc_train, by = "patient_num")
```


#### Rearranging columns order
```{r}
med_train <- med_train[,c(1,3,2,4)]

proc_train <- proc_train[,c(1,3,4,2,5)]
```



#### Calculating the date intervals in days

    define LOS = discharge_date - admission_date

```{r}
demo_train <- demo_train %>% 
  mutate(
    los = as.numeric(difftime(discharge_date, admission_date, units ="days"))
  )
```

```{r}
med_train <- med_train %>% 
  mutate(
    days_med_adm = as.numeric(difftime(medication_date, admission_date, units ="days"))
  )
```

```{r}
proc_train <- proc_train %>% 
  mutate(
    days_pro_adm = as.numeric(difftime(procedure_date, admission_date, units ="days"))
  )
```


#### Checking Missingness

    No missing data in all three training datasets. 
    
```{r}
demo_train[!complete.cases(demo_train),]  ## no missing data
```
```{r}
proc_train[!complete.cases(proc_train),] ## no missing data
```
```{r}
med_train[!complete.cases(med_train),] # no missing data
```


####  Data Visualization 

* Age 

```{r}
ggplot(data = demo_train) + geom_histogram(mapping=aes(x=age), binwidth=0.5)
```

* Race by Death

```{r}
ggplot(demo_train, aes(x = death, fill = race)) +
  geom_bar() + 
  scale_fill_viridis_d()
```




#### Descriptive of demographic training data

```{r}
demo_train$death <- as.factor(demo_train$death)
cg <- compareGroups(death ~ gender+race+age+los, 
                    data = demo_train, 
                    method = c(3,3,1,2), 
                    max.xlev = 12, 
                    Q1 = 0, Q3 = 1)
createTable(cg, show.n = F, show.p.overall = T)
```

    - delete the one observation that has unknown gender in demo train
    
    - race has too many categories, need to combine some of them. 
    
      + Combine the minority groups Asian, Pacific Islander, Asian/Pacific Islander, Biracial, Hispanic, Mid Eastern Indian, Native American, and Other to a new category, Others. 
      
      + We may want to keep the Unknown category in race. (Are Unknowns missing values?) //Steph: good question, I would think yes, they are missing values. 
      
    - delete those patients discharged within first 3 days (los < 3)
    
     + 786 patients were deleted. 
  
```{r}
demo_train_new <- demo_train %>% filter(gender != "Unknown") %>%
  mutate(race = fct_recode(race, 
                           `Others` = 'Asian', 
                           `Others` = 'Pacific Islander', 
                           `Others` = 'Asian/Pacific Islander', 
                           `Others` = 'Biracial', 
                           `Others` = 'Hispanic', 
                           `Others` = 'Mid Eastern Indian', 
                           `Others` = 'Native American', 
                           `Others` = 'Other')) %>%
  filter(los >= 3)
  
```
 
 
#### Descriptive analysis for new demographic training data: 
```{r}
cg.new <- compareGroups(death ~ gender+race+age+los, 
                    data = demo_train_new, 
                    method = c(3,3,1,2), 
                    Q1 = 0, Q3 = 1)
createTable(cg.new, show.n = F, show.p.overall = T)
```

```{r}
# library(mice)
# demo.tmp2
# demo.imputed <- mice(demo.tmp2, method = "polyreg", seed = 123, 
#                      printFlag = F, 
#                      formulas = list(race ~ age + gender))
```

```{r}
# There are 5 imputed data, select only of them for descriptive analysis.
# demo_imputed <- complete(demo.imputed, 1) 
# cg.new2 <- compareGroups(death ~ gender+race+age+los, 
#                     data = demo_imputed, 
#                     method = c(3,3,1,2), 
#                     Q1 = 0, Q3 = 1)
# createTable(cg.new2, show.n = F, show.p.overall = T)
```

Deleting Unknowns in race variable since the missingness is <10%. 

```{r}
demo.tmp <- demo_train_new %>% filter(race != "Unknown")
cg.new <- compareGroups(death ~ gender+race+age+los, 
                    data = demo.tmp, 
                    method = c(3,3,1,2), 
                    Q1 = 0, Q3 = 1)
createTable(cg.new, show.n = F, show.p.overall = T)
```


* Plot of LOS
```{r}
demo_train_new<- demo_train_new %>% filter(race != "Unknown")
```

```{r}
ggplot(demo_train_new, aes(los, fill = death)) + 
  geom_histogram(alpha = 0.5, binwidth = 10) + 
  theme_bw()
```

##### Descriptives of medication training data

New variables' definition/Codebook:
    
    n_med: number of medications per patient.
    
    min_time_from_admit, max_time_from_admit: minimal/maximum time of medication administration after admission to hospital for each patient. 
    
    any_vaso: Did this patient receive at least 1 vassopressor (dopamin, phenylephrine, norepinephrine)?
    
    n_vaso: Number of vassopressors the patient took. 
    
```{r message=FALSE}
med.tmp1 <- med_train %>% group_by(patient_num) %>%
  summarise(n_med = n(),
            min_days_med_adm = min(days_med_adm),
            max_days_med_adm = max(days_med_adm),
            any_vaso = any(generic_name %in% c("dopamine", "phenylephrine",  "norepinephrine")),
            n_vaso = sum(generic_name %in% c("dopamine", "phenylephrine",  "norepinephrine"))) %>% 
  ungroup()
```


```{r}
cg2 <- compareGroups(~ n_med + n_med + min_days_med_adm + max_days_med_adm + any_vaso + n_vaso, 
                     data = med.tmp1, 
                     method = c(1,2,2,2,3,2), Q1 = 0, Q3 = 1)
createTable(cg2, show.n = F)
```

    The number of medications administered to a patient averages at 26 (median = 23) and ranges from 1 to 182. The majority of medications were administered within one day of admission to the hospital. About 18.1% of patient received at lease one vassopressor. 

#### Descriptive analysis grouped by any_vaso:

```{r}
med.tmp1$any_vaso <- as.factor(med.tmp1$any_vaso)
cg3 <- compareGroups(any_vaso ~ n_med + min_days_med_adm + max_days_med_adm + n_vaso , 
                     data = med.tmp1, 
                     method = c(1,2,2,2,2,1,2), Q1 = 0, Q3 = 1)
createTable(cg3, show.n = F)
```

    The table above shows that patients who have at least taken one vasopressors tend to have taken higher number of medications and larger range of medication administration time. 

```{r message=FALSE}
med_freq <- med_train %>% group_by(generic_name) %>%
  summarise(n = n()) %>%
  arrange(desc(n)) 

med_freq
```

DOUBLE CHECK:: We are planning to create dummy variables for each medication that had been prescribed to at least 500 patients in the dataset.


#### Descriptives analysis for procedure training data

```{r}
proc.tmp1 <- proc_train %>% group_by(patient_num) %>% 
  summarise(n_proc = n(), 
            min_days_pro_adm = min(days_pro_adm),
            max_days_pro_adm = max(days_pro_adm)) 

proc.tmp1
```


```{r}
cg3 <- compareGroups(~ n_proc + n_proc + min_days_pro_adm + max_days_pro_adm, 
                     data = proc.tmp1, 
                     method = c(1,2,2,2), 
                     Q1 = 0, Q3 = 1)
createTable(cg3, show.n = F)
```



#### View frequently used procedured by procedure_id:

    There's a total of 1254 procedures. The top 3 most used procedure is arteriography of cerebral arteries, minsertion of endotracheal tube followed by continuous invasive mechanical ventilation for less than 96 consecutive hours.

```{r}
proc_freq_testing <- proc_train %>% group_by(procedure_description) %>%
  summarise(n = n(), procedure_description = first(procedure_description)) %>%
  arrange(desc(n)) 

proc_freq_testing
```

```{r}
proc_freq <- proc_train %>% group_by(procedure_id) %>%
  summarise(n = n(), 
            procedure_description = first(procedure_description)) %>%
  arrange(desc(n)) 

proc_freq
```


#### Combining all dataframes into one by id and admission date (WHEN THE SMALLER DATASETS ARE CLEANED)
```{r}
joined_df <- demo_train_new %>%
  left_join(med.tmp1, by = c('patient_num')) %>%
  left_join(proc.tmp1, by = c('patient_num'))
```

#### Pivot med and proc dfs and merge with joined_df to get individual meds/procedures as features

```{r}
# filter to only medications with 50 or more patients
med_freq_filter = filter(med_freq, n>50) # 168 meds with 50 or more patients
#remove cols from med df so we get one row per patient after pivoting
med_train = subset(med_train, select=-c(admission_date, medication_date))
# pivot med df to get one row per patient
med_train_pivot = med_train %>%
  group_by(patient_num, generic_name) %>%
  filter(row_number() == 1) %>%
  mutate(n = 1) %>%
  pivot_wider(id_cols = patient_num, names_from = generic_name, values_from = n, values_fill = list(n = 0))
  #pivot_wider(names_from = generic_name, values_from = days_med_adm, values_fn = mean)
#select subset of med pivot df with only medications given to 50 or more patients
med_train_pivot_filter = subset(med_train_pivot,select= c('patient_num',med_freq_filter$generic_name))
```

##### repeat above 4 steps for procedures df
```{r}
proc_freq_filter = filter(proc_freq, n>50) # 59 procs with > 50 patients
proc_train = subset(proc_train, select=-c(procedure_description,admission_date, procedure_date))
proc_train_pivot = proc_train %>% 
  group_by(patient_num, procedure_id) %>%
  filter(row_number() == 1) %>%
  mutate(n = 1) %>%
  pivot_wider(id_cols = patient_num, names_from = procedure_id, values_from = n, values_fill = list(n = 0))
  # pivot_wider(names_from = procedure_id, values_from = days_pro_adm, values_fn = mean)
proc_train_pivot_filter = subset(proc_train_pivot,select= c('patient_num',proc_freq_filter$procedure_id))
```

##### merge into master df
```{r}
merged_train_df = joined_df %>%
  left_join(med_train_pivot_filter, by = c('patient_num')) %>% 
   left_join(proc_train_pivot_filter, by = c('patient_num'))
```

##### make final train set

```{r}
#convert NAs to zeros
merged_train_df[is.na(merged_train_df)] = 0

#admission/discharge dates to get final training set 
train_final = subset(merged_train_df,select= -c(admission_date,discharge_date))
```

#### Save final train data into "train_final.RData"

```{r}
save(train_final, file = "train_final.RData")
```

#### load data
```{r}
load("train_final.RData")
```

#### a quick descriptive
```{r}
tb <- createTable(compareGroups(death ~. , data = train_final[, -1]))
export2word(tb, file = "train_final_descr.doc")
```

##### convert categorical variables to factors
```{r}
train_final$gender = as.factor(train_final$gender)
train_final$any_vaso = as.factor(train_final$any_vaso)
```

##### convert response variable to binary
```{r}
train_final$death = as.integer(as.logical(train_final$death))
```

```{r}
#train_final = train_final %>% select(-patient_num, -los)
train_final = subset(train_final, select=-c(patient_num, los))
```


### split training data into training and validation sets
```{r}
set.seed(3)
n=nrow(train_final)
train.index=sample(1:n, n*0.75)
train=train_final[train.index,]
validation=train_final[-train.index,]
```


### Dimension reduction

#### Variable Selection
```{r}
var.names <- names(train_final)[-which(names(train_final) == "death")]
var.pvalues <- foreach(i = 1:length(var.names), .combine = c) %do% {
  fmla <- as.formula(paste0("death~", var.names[i]))
  fit <- glm(fmla, data = train_final, family = binomial)
  s <- summary(fit)
  s$coefficients[2,4]
}
```

```{r}
var.df <- data.frame(names = var.names, pval = var.pvalues)
```

```{r}
# var.sig.level005 <- var.df %>% filter(pval <= 0.05) %>% arrange(pval)
var.sig <- var.df %>% filter(pval <= 0.1) %>% arrange(pval)
```

```{r}
head(var.sig, 50)
```
```{r}
save(var.sig, file = "var.sig.RData")
```


### Predictive models

#### 1. Logistic regression (without Dimension reduction)

    AUC 0.8612885 
    Prediction 0.8275862        
    glm.pred   0   1
            0 813 126
            1  79 171

```{r message=FALSE}
# simple logistic regression with 5-fold cross validation
default_glm_mod=train(
  form=death~.,
  data=train,
  trControl=trainControl(method="cv", number=5),
  method="glm",
  family='binomial'
)
# default_glm_mod
# default_glm_mod$results
# default_glm_mod$finalModel
# summary(default_glm_mod) 

# AUC 
glm.probs=predict(default_glm_mod, newdata=validation) ## probability
roc_obj <- roc(validation$death, glm.probs)
auc = auc(roc_obj)
cat('\n','AUC', auc, '\n')

# Accuracy 
glm.pred=ifelse(glm.probs>0.5, 1, 0)
val.accuracy = mean(validation$death == glm.pred)
cat('Accuracy', val.accuracy)
#confusion matrix
table(glm.pred, validation$death)
```


#### 2. Logistic Regression (with Reduced variables - names in "var.sig.RDate")

```{r}
#renaming some column names
# get names from var.sig in a list : dput(as.character(var.sig$names))
edited_var <- c("P141", "P44", "norepinephrine", "age", "P2549", "ondansetron", 
"P154650", "nimodipine", "P122879", "P2548", "any_vaso", "piperacillin_tazobactam", 
"hydralazine", "heparin", "lidocaine", "ocular_lubricant", "dopamine", 
"fentanyl", "glucose", "n_vaso", "docusate", "labetalol", "etomidate", 
"mannitol", "min_days_med_adm", "P43", "P4852", "codeine", "hydromorphone", 
"sodium_bicarbonate", "lvp_solution_with_hypertonic_saline", 
"cefepime", "P310", "P4631", "P3006", "lorazepam", "acetaminophen_oxycodone", 
"chlorhexidine_topical", "succinylcholine", "apap_butalbital_caffeine", 
"P161794", "P161793", "min_days_pro_adm", "P4756", "sterile_water", 
"albuterol_ipratropium", "ipratropium", "P141780", "protamine", 
"levofloxacin", "n_proc", "acetaminophen_hydrocodone", "levetiracetam", 
"vancomycin", "metoclopramide", "potassium_phosphate_sodium_phosphate", 
"ceftriaxone", "promethazine", "nicardipine", "max_days_med_adm", 
"P337", "lvp_solution_with_potassium", "lvp_solution", "glucagon", 
"vecuronium", "oxycodone", "P2232", "glycopyrrolate", "dexamethasone", 
"calcium_chloride", "P42", "sodium_chloride", "diphenhydramine", 
"propofol", "ketorolac", "P157534", "magnesium_sulfate", "phytonadione", 
"acetaminophen", "digoxin", "neostigmine", "verapamil", "P356", 
"simvastatin", "ranitidine", "P159871", "P284", "albuterol", 
"methylprednisolone", "pravastatin", "insulin_regular", "midazolam", 
"meperidine", "sugammadex", "morphine", "clindamycin", "senna", 
"furosemide", "P5097", "P606", "P2610", "aspirin", "phenytoin", 
"remifentanil", "papaverine", "insulin_aspart_insulin_aspart_protamine", 
"P90", "calcium_carbonate", "hydrochlorothiazide", "clonidine", 
"docusate_senna", "indocyanine_green", "iopamidol", "P155690", 
"prochlorperazine", "P157536", "acetaminophen_codeine", "nicotine", 
"n_med", "calcium_gluconate", "P157076", "P620", "race", "sufentanil", 
"insulin_lispro", "bisacodyl", "iodixanol", "eptifibatide", "losartan", 
"P159447", "zolpidem", "P158020", "P157074", "lisinopril", "P110148", 
"esmolol", "dolasetron", "al_hydroxide_mg_hydroxide_simethicone", 
"gender", "P84", "heparin_flush", "fluticasone_nasal", "amlodipine", 
"P98597", "cefazolin", "rosuvastatin", "lidocaine_topical")
```

Logistic regression with reduced variable set 1 (var.sig)

    AUC 0.8658
    Prediction 0.8334735       
    glm.pred   0   1
            0 826 132
            1  66 165

    
```{r}
train.reduced_1 <- train %>% select(edited_var,death)
validation.reduced_1 <- validation %>% select(edited_var,death)

default_glm_mod_1=train(
  form=death~.,
  data=train.reduced_1,
  trControl=trainControl(method="cv", number=5),
  method="glm",
  family='binomial'
)

# AUC 
glm.probs=predict(default_glm_mod_1, newdata=validation.reduced_1) ## probability
roc_obj <- roc(validation.reduced_1$death, glm.probs)
auc = auc(roc_obj)
cat('AUC', auc, '\n')

# Accuracy 
glm.pred=ifelse(glm.probs>0.5, 1, 0)
val.accuracy = mean(validation.reduced_1$death == glm.pred)
cat('Accuracy', val.accuracy)
#confusion matrix
table(glm.pred, validation.reduced_1$death)
```

##### with reduced variables from forward.res.RData
```{r}
red.var <- c("age", "P44", "P141", "P154650", "norepinephrine", "P2549", 
"P2548", "P161794", "P161793", "P4852", "P43", "ocular_lubricant", 
"mannitol", "propofol", "cefepime", "P157534", "lvp_solution", 
"n_med", "piperacillin_tazobactam", "P310", "P337", "codeine", 
"P620", "lvp_solution_with_hypertonic_saline", "sodium_bicarbonate", 
"iopamidol", "apap_butalbital_caffeine", "calcium_carbonate", 
"P4631", "P284", "sterile_water", "P157076", "iodixanol", "glucose", 
"glucagon")
```

    Accuracy: 0.84020
            
    glm.pred   0   1
           0 836 143
           1  56 163
    Setting levels: control = 0, case = 1
    Setting direction: controls < cases
    Area under the curve: 0.8747
    
```{r}
train.reduced_2 <- train %>% select(red.var,death)
validation.reduced_2 <- validation %>% select(red.var,death)

default_glm_mod_2=train(
  form=death~.,
  data=train.reduced_2,
  trControl=trainControl(method="cv", number=5),
  method="glm",
  family='binomial'
)

# AUC 
glm.probs=predict(default_glm_mod_2, newdata=validation.reduced_2) ## probability
roc_obj <- roc(validation.reduced_2$death, glm.probs)
auc = auc(roc_obj)
cat('AUC', auc, '\n')

# Accuracy 
glm.pred=ifelse(glm.probs>0.5, 1, 0)
val.accuracy = mean(validation.reduced_2$death == glm.pred)
cat('Accuracy', val.accuracy)
#confusion matrix
table(glm.pred, validation.reduced_2$death)
```

#### 3. Gradient Boosting

```{r}
#make dummy vars from categorical vars
train.dummy = dummyVars("~.",data=train)
train.onehot = data.frame(predict(train.dummy,newdata=train))
validation.dummy = dummyVars("~.",data=validation)
validation.onehot = data.frame(predict(validation.dummy,newdata=validation))
```

#grid search cv (Runtime of this block 1-2 hours)
```{r}
xgb <- XGBTrainer$new(objective = "binary:logistic", eval_metric = 'auc')

gst <-GridSearchCV$new(trainer = xgb,
                             parameters = list(n_estimators = c(100,500,1000), 
                                               learning_rate = c(.01,.1,.5),
                                               max_depth = c(2,4,6),
                                               subsample= c(0.5,1)
                                               ),
                             n_folds = 10,
                             scoring = c('auc'))
gst$fit(train.onehot, "death")
gst$best_iteration()
```

XGBoost model run on all features using optimized hyperparameters from grid search cv
```{r}
#fit model
boost = gbm(death~.,data=train,distribution='bernoulli',n.trees=1000,shrinkage=0.01, interaction.depth=4,class.stratify.cv = 5)
#get predicted probabilities
train.probs = predict(boost, train, n.trees = 1000)
val.probs = predict(boost, validation, n.trees = 1000)
#get binary predictions
train.pred=ifelse(train.probs>0.5, 1, 0)
val.pred=ifelse(val.probs>0.5, 1,0)
#get accuracy 
train.accuracy = mean(train.pred==train$death)
val.accuracy = mean(val.pred==validation$death)
train.accuracy
val.accuracy
#confusion matrix
table(val.pred, validation$death)
#auc
labels=factor(validation$death)
pROC::auc(roc(labels,val.probs))
```

XGboost full model feature importance
```{r}
summary(boost)
```

XGBoost backward feature selection
```{r}
#get list of top features from model summary
boost.top.features = summary(boost)[,1:2]
boost.top.features = gsub('`','',boost.top.features[,1])

n.features=seq(230,10,-20) #defines range of number of features to test
boost.auc=rep(0,length(n.features)) #empty array to add results

#run Boost model with reduced feature sets using each value in n.features
for(i in 1:length(n.features)){
  boost.reduced = gbm(death~.,data=train[,c('death',boost.top.features[1:n.features[i]])],distribution='bernoulli',n.trees=1000,shrinkage=0.01, interaction.depth=4, class.stratify.cv = 5)
  val.probs = predict(boost.reduced, validation[,c('death',boost.top.features[1:n.features[i]])], n.trees = 1000)
  boost.auc[i] = pROC::auc(roc(labels,val.probs))
}
#plot AUC vs. n.features
plot(n.features,boost.auc,type="b", xlim=rev(range(n.features)))
```

fit Boost final model with optimized n.features (n=70)
```{r}
#get list of top features from model summary
boost.top.features = summary(boost)[,1:2]
boost.top.features = gsub('`','',boost.top.features[,1])

#fit model
boost.final = gbm(death~.,data=train[,c('death',boost.top.features[1:70])],distribution='bernoulli',n.trees=1000,shrinkage=0.01, interaction.depth=4,
            class.stratify.cv = 5)
#get predicted probabilities
train.probs = predict(boost.final, train[,c('death',boost.top.features[1:70])], n.trees = 1000)
val.probs = predict(boost.final, validation[,c('death',boost.top.features[1:70])], n.trees = 1000)
#get binary predictions
train.pred=ifelse(train.probs>0.5, 1, 0)
val.pred=ifelse(val.probs>0.5, 1,0)
#get accuracy 
train.accuracy = mean(train.pred==train$death)
val.accuracy = mean(val.pred==validation$death)
train.accuracy
val.accuracy
#confusion matrix
table(val.pred, validation$death)
#auc
#plot(roc(val.probs,labels))
labels=factor(validation$death)
pROC::auc(roc(labels,val.probs))
```

Boost final model feature importance
```{r}
summary(boost.final)
```


#### 4. Random Forest

Fit random forest using grid search cv to find optimized hyperparameters
```{r}
rf <- RFTrainer$new(classification=1,verbose=FALSE)

gsrf <-GridSearchCV$new(trainer = rf,
                             parameters = list(n_estimators = c(10,100,300,500),
                                               max_features = c(3,15,60,120)
                                               ),
                             n_folds = 5,
                             scoring = c('auc'))
gsrf$fit(X=train.onehot, y="death")
gsrf$best_iteration()
```

Fit full random forest model using optimized hyperparameters
```{r}
# create x and y training and test sets
x.train = train[,-4]
y.train = as.factor(train$death)
x.val = validation[,-4]
y.val = as.factor(validation$death)

#fit random forest for each value of mtry
rf.p2=randomForest(x.train, y.train, xtest = x.val, ytest = y.val, mtry = 15, ntree = 100)

#plot test error for each model
plot(1:100, rf.p2$test$err.rate[,1], col = "red", type = "l", xlab = "Number of Trees", ylab = "Test Err Rate", ylim = c(.15, .25),main='Test Error Rate vs. Number of Trees')

# Evaluate model accuracy
#rf.p2$test$err.rate #Shows error % at each of 500 trees 
rf.p2$test$confusion # Confusion matrix
#rf.p2$test$votes #Gives predicted probability for each sample in test set
#rf.p2$test$votes[,2] #Gives predicted probability for y=1

#get feature importance 
rf.top.features = importance(rf.p2)
#varImpPlot(rf.p2)

### Compute AUC and plot ROC-AUC curve ###
pROC::auc(roc(y.val,rf.p2$test$votes[,2]))
#auc(roc(rf.p2$test$votes[,2],y.val)) #get AUC score
#plot(roc(rf.p2$test$votes[,2],y.test)) #plot ROC-AUC curve
```

RF backward feature selection
```{r}
#get list of top features from model summary
rf.top.features = importance(rf.p2)
rf.top.features=as.data.frame(rf.top.features)
rf.top.features = cbind(feature = rownames(rf.top.features), rf.top.features)
rf.top.features = rf.top.features[order(-rf.top.features[,2]),]

n.features=seq(230,10,-20) #defines range of number of features to test
rf.auc=rep(0,length(n.features)) #empty array to add results

#run Boost model with reduced feature sets using each value in n.features
for(i in 1:length(n.features)){
  rf.reduced = randomForest(x.train[,c(rf.top.features[,1][1:n.features[i]])], y.train, 
                            xtest = x.val[,c(rf.top.features[,1][1:n.features[i]])], ytest = y.val, mtry = 15, ntree = 100)
  rf.auc[i] = auc(roc(rf.reduced$test$votes[,2],y.val))
}
#plot AUC vs. n.features
plot(n.features,rf.auc,type="b", xlim=rev(range(n.features)))
```

Fit reduced random forest model with optimized n.features=110
```{r}
#fit random forest
rf.reduced=randomForest(x.train[,c(rf.top.features[,1][1:110])], y.train, 
                   xtest = x.val[,c(rf.top.features[,1][1:110])], ytest = y.val, 
                   mtry = 15, ntree = 100)

#plot test error for each model
plot(1:100, rf.reduced$test$err.rate[,1], col = "red", type = "l", xlab = "Number of Trees", ylab = "Test Err Rate", ylim = c(.15, .25),main='Test Error Rate vs. Number of Trees')

# Evaluate model accuracy
rf.reduced$test$confusion # Confusion matrix

#get feature importance 
rf.top.features.reduced = importance(rf.reduced)
#varImpPlot(rf.p2)

### Compute AUC ###
#auc(roc(rf.p2$test$votes[,2],y.val)) #get AUC score
pROC::auc(roc(y.val,rf.p2$test$votes[,2]))

```

SMOTE for unbalanced classes - oversampling to balance classes
```{r}
#SMOTE to balance classes - training set only
train.balanced = smotefamily::SMOTE(X=train.onehot[,-7],target=train.onehot[,7], dup_size = 2)
train.balanced = train.balanced$data
train.balanced = train.balanced %>% rename(death = class)
table(train.balanced$death)
table(train$death)
```

Rerun xgb on balanced data

gradient boost

    accuracy = 0.812
    boosting still predicting very few positive outcomes

```{r}
#fit model
boost.bal = gbm(death~.,data=train.balanced,distribution='bernoulli',n.trees=1000,shrinkage=0.01, interaction.depth=4, class.stratify.cv = 5)
#get predicted probabilities
train.probs = predict(boost.bal, train.balanced, n.trees = 1000)
val.probs = predict(boost.bal, validation.onehot, n.trees = 1000)
#get binary predictions
train.pred=ifelse(train.probs>0.5, 1, 0)
val.pred=ifelse(val.probs>0.5, 1,0)
#get accuracy 
train.accuracy = mean(train.pred==train.balanced$death)
val.accuracy = mean(val.pred==validation.onehot$death)
train.accuracy
val.accuracy
#confusion matrix
table(val.pred, validation.onehot$death)
#auc
labels=factor(validation$death)
pROC::auc(roc(labels,val.probs))
```


.Tree
```{r}
#Classification Tree with rpart
library(rpart)
# grow tree
fit <- rpart(death~., data=train, method='class')
printcp(fit) # display the results
plotcp(fit) # visualize cross-validation results
summary(fit) # detailed summary of splits

# plot tree
plot(fit, uniform=TRUE,
   main="Classification Tree for Death")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
```

#### 5. Lasso 

```{r}
# move death to the last column
train.data <- train %>% relocate(death, .after=last_col())
test.data <- validation %>% relocate(death, .after=last_col())
```

```{r}
# rearranging columns
x <- model.matrix(death~., train.data)[,-1]
y <- (train.data$death)
```

fit the lasso penalized regression model 
```{r}
#fit the lasso penalized regression model
set.seed(3) 
cv.lasso <- cv.glmnet(x, y, alpha = 1, family = "binomial", nfolds=5)
```

compute the final lasso model using lambda.min (Accuracy: 0.8359966):
```{r}
# compute the final model using lambda.min:
# Final model with lambda.min
lasso.model <- glmnet(x, y, alpha = 1, family = "binomial",
                      lambda = cv.lasso$lambda.min)
# Make prediction on test data
x.test <- model.matrix(death ~., test.data)[,-1]

probabilities <- lasso.model %>% predict(newx = x.test, type="response")
predicted.classes <- ifelse(probabilities > 0.5, 1, 0)
# Model accuracy
observed.classes <- test.data$death
mean(predicted.classes == observed.classes) 
table(predicted.classes, test.data$death)
# AUC
roc_obj <- roc(test.data$death, probabilities)
auc = auc(roc_obj)
cat('AUC with lambda.min', auc)
```

compute the final model using lambda.lse (Accuracy: 0.8301093):
```{r}
# Final model with lambda.1se
lasso.model <- glmnet(x, y, alpha = 1, family = "binomial",
                      lambda = cv.lasso$lambda.1se)
# Make prediction on test data
x.test <- model.matrix(death ~., test.data)[,-1]
probabilities <- lasso.model %>% predict(newx = x.test, type="response")
predicted.classes <- ifelse(probabilities > 0.5, 1, 0)
# Model accuracy rate
observed.classes <- test.data$death
mean(predicted.classes == observed.classes)
table(predicted.classes, test.data$death)
# AUC
roc_obj <- roc(test.data$death, probabilities)
auc = auc(roc_obj)
cat('AUC with lambda.lse', auc)
```


#### 6. Super Learner
Individual model for glm
```{r}
set.seed(1)
sl = SuperLearner(Y=train.reduced$death, X=train.reduced[,-148], family = binomial(), SL.library = "SL.glm")
sl
```

AUC (0.8603 for train, 0.8223 for validation)
```{r}
# onlySL: only fit models that had weight != 0.
# train auc
pred=predict(sl, train.reduced[,-148], onlySL = TRUE)
pred_rocr = ROCR::prediction(pred$pred, train.reduced$death)
auc = ROCR::performance(pred_rocr, measure = "auc", x.measure = "cutoff")@y.values[[1]]
auc

# validation auc
pred1=predict(sl, validation.reduced[,-148], onlySL = TRUE)
pred_rocr1 = ROCR::prediction(pred1$pred, validation.reduced$death)
auc1 = ROCR::performance(pred_rocr1, measure = "auc", x.measure = "cutoff")@y.values[[1]]
auc1
```

Individual model for gbm
```{r}
set.seed(1)
sl = SuperLearner(Y=train$death, X=train, SL.library = "SL.gbm")
```

Multiple model fit
```{r}
set.seed(1)
sl = SuperLearner(Y=train$death, X=train, family = binomial(),
                  SL.library =c("SL.gbm","SL.glm"))
```

```{r}
pred=predict(sl, validation, onlySL = TRUE)
pred_rocr = ROCR::prediction(pred$pred, validation.reduced$death)
auc = ROCR::performance(pred_rocr, measure = "auc", x.measure = "cutoff")@y.values[[1]]
auc
```

#### 6. SVM

a.	Data preparation
We split the data into training and validation sets.
```{r}
load("train_final.RData")

dropvar = names(train_final) %in% c('los','patient_num')
train_final = train_final[!dropvar]

train_final$death = as.factor(train_final$death)


set.seed(3)
n=nrow(train_final)
train.index=sample(1:n, n*0.75)
train=train_final[train.index,]
validation=train_final[-train.index,]


```

b. Fit a support vector classifier to the data with various values of cost and select the best one.
Note that CV is used for building the model for the training data. 

```{r}
# linear
# single

svm.linear <- svm(death ~ ., data = train_final, kernel = "linear", cost = 1,decision.values=T, probability =T)


pred <- predict(svm.linear, train, decision.values = TRUE)
#head(pred)

pred_roc <- attributes(pred)$decision.values
#pred_roc <- attributes(pred)$probabilities
#head(pred_roc)

pred <- as.numeric(as.character(pred))

confusion <- table(pred, train$death)
confusion

accuracy <- (confusion[1,1] + confusion[2,2])/sum(confusion)

install_load('ROCR')

rocplot=function(pred, truth, ...){
 predob = prediction(pred, truth, c(1,0))
 perf = performance(predob, "tpr", "fpr")
 auc = performance(predob, 'auc')
 
   plot(perf,...)
   return(attributes(auc)$y.values[[1]])}

train_auc = rocplot(pred_roc ,train$death, main=" Training Data")
train_auc

val.pred <- predict(svm.linear, validation, decision.values = TRUE)
#head(val.pred)
val.pred_roc = attributes(val.pred)$decision.values
val.pred <- as.numeric(as.character(val.pred))


val.confusion <- table(val.pred, validation$death)
val.confusion
val.accuracy <- (val.confusion[1,1] + val.confusion[2,2])/sum(val.confusion)

val.auc = rocplot(val.pred_roc ,validation$death, main="Validation Data")
val.auc


tr = c(accuracy,train_auc)
val = c(val.accuracy,val.auc)

cb = as.data.frame(rbind(tr,val))
colnames(cb) = c("Accuracy","AUC")
rownames(cb) = c("Training","Validation")

kable(cb,caption = "SVM linear kernel",digits = 3)  


TPR = val.confusion[2,2]/(val.confusion[1,2]+val.confusion[2,2])
TPR
TNR = val.confusion[1,1]/(val.confusion[1,1]+val.confusion[2,1])
TNR

```





```{r, eval=F}
# tune
set.seed(12)
linear.out <- tune(svm, death ~ ., data = train, kernel = "linear", ranges = list(cost = c(0.1, 1, 5)))
summary(linear.out)
save(linear.out,file = "SVM_linear.Rdata")


set.seed(12)
linear.out2 <- tune(svm, death ~ ., data = train, kernel = "linear", ranges = list(cost = c(0.001,0.01, 0.1, 10, 100, 1000)))
save(linear.out2,file = "SVM_linear2.Rdata")

summary(linear.out2)


```




```{r}
# polynomial
# single

# 10 fold CV error from on the training data: 0.1988454


svm.poly <- svm(death ~ ., data = train, kernel = "polynomial", cost = 1, degree = 2)
#svm.poly

pred <- predict(svm.poly, train, decision.values = TRUE)
#head(pred)

pred_roc <- attributes(pred)$decision.values
#pred_roc <- attributes(pred)$probabilities
#head(pred_roc)

pred <- as.numeric(as.character(pred))

confusion <- table(pred, train$death)
confusion

accuracy <- (confusion[1,1] + confusion[2,2])/sum(confusion)

install_load('ROCR')

rocplot=function(pred, truth, ...){
 predob = prediction(pred, truth, c(1,0))
 perf = performance(predob, "tpr", "fpr")
 auc = performance(predob, 'auc')
 
   plot(perf,...)
   return(attributes(auc)$y.values[[1]])}

train_auc = rocplot(pred_roc ,train$death, main=" Training Data")
train_auc

val.pred <- predict(svm.poly, validation, decision.values = TRUE)
#head(val.pred)
val.pred_roc = attributes(val.pred)$decision.values
val.pred <- as.numeric(as.character(val.pred))


val.confusion <- table(val.pred, validation$death)
val.confusion
val.accuracy <- (val.confusion[1,1] + val.confusion[2,2])/sum(val.confusion)

val.auc = rocplot(val.pred_roc ,validation$death, main="Validation Data")
val.auc


tr = c(accuracy,train_auc)
val = c(val.accuracy,val.auc)

cb = as.data.frame(rbind(tr,val))
colnames(cb) = c("Accuracy","AUC")
rownames(cb) = c("Training","Validation")

kable(cb,caption = "SVM polynomial kernel",digits = 3)  


TPR = val.confusion[2,2]/(val.confusion[1,2]+val.confusion[2,2])
TPR
TNR = val.confusion[1,1]/(val.confusion[1,1]+val.confusion[2,1])
TNR

```



```{r, eval=F}
# tune
set.seed(12)
poly.tune.out <- tune(svm, death ~ ., data = train,  kernel = "polynomial", ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100), degree = c(2, 3, 4)))
summary(poly.tune.out)

save(poly.tune.out,file = "SVM_polynomial.Rdata")
```


```{r}

# radial:
# single

# 10 fold CV error from on the training data: 0.1894415

svm.radial <- svm(death ~ ., data = train,kernel = "radial", cost = 10, gamma = 0.001)
#svm.radial

pred <- predict(svm.radial, train, decision.values = TRUE)
#head(pred)

pred_roc <- attributes(pred)$decision.values
#pred_roc <- attributes(pred)$probabilities
#head(pred_roc)

pred <- as.numeric(as.character(pred))

confusion <- table(pred, train$death)
confusion

accuracy <- (confusion[1,1] + confusion[2,2])/sum(confusion)

install_load('ROCR')

rocplot=function(pred, truth, ...){
 predob = prediction(pred, truth, c(1,0))
 perf = performance(predob, "tpr", "fpr")
 auc = performance(predob, 'auc')
 
   plot(perf,...)
   return(attributes(auc)$y.values[[1]])}

train_auc = rocplot(pred_roc ,train$death, main=" Training Data")
train_auc

val.pred <- predict(svm.radial, validation, decision.values = TRUE)
#head(val.pred)
val.pred_roc = attributes(val.pred)$decision.values
val.pred <- as.numeric(as.character(val.pred))


val.confusion <- table(val.pred, validation$death)
val.confusion
val.accuracy <- (val.confusion[1,1] + val.confusion[2,2])/sum(val.confusion)

val.auc = rocplot(val.pred_roc ,validation$death, main="Validation Data")
val.auc


tr = c(accuracy,train_auc)
val = c(val.accuracy,val.auc)

cb = as.data.frame(rbind(tr,val))
colnames(cb) = c("Accuracy","AUC")
rownames(cb) = c("Training","Validation")

kable(cb,caption = "SVM radial kernel",digits = 3)  


TPR = val.confusion[2,2]/(val.confusion[1,2]+val.confusion[2,2])
TPR
TNR = val.confusion[1,1]/(val.confusion[1,1]+val.confusion[2,1])
TNR


```

```{r eval=F}
# tune
set.seed(12)

radial.tune.out <- tune(svm, death ~ ., data = train, kernel = "radial", ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100), gamma = c(0.01, 0.1, 1, 5, 10, 100)))
summary(radial.tune.out)
save(radial.tune.out,file = "SVM_radial.Rdata")

# 10 fold CV error from on the training data: 0.1952782
# cost = 1 gamma = 0.01

# 2nd round
radial.tune.out2 <- tune(svm, death ~ ., data = train, kernel = "radial", ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 100), gamma = c(0.0001,0.001, 0.01)))
summary(radial.tune.out2)
save(radial.tune.out2,file = "SVM_radial2.Rdata")

# 3rd round
radial.tune.out3 <- tune(svm, death ~ ., data = train, kernel = "radial", ranges = list(cost = c( 10,100,500,1000), gamma = c(0.000001,0.00001,0.0001,0.001)))
summary(radial.tune.out3)
save(radial.tune.out3,file = "SVM_radial3.Rdata")

```


### Get final test set predictions
```{r}
#fit GBM model on full training set with optimized parameters
boost.test = gbm(death~.,data=train_final[,c('death',boost.top.features[1:70])],distribution='bernoulli',n.trees=1000,shrinkage=0.01, interaction.depth=4, class.stratify.cv = 5)
#get predicted probabilities
boost.probs = predict(boost.test, train_final[,c('death',boost.top.features[1:70])], n.trees = 1000, type='response')
#get binary predictions
boost.pred=ifelse(boost.probs>0.5, 1, 0)
#get accuracy 
boost.accuracy = mean(boost.pred==train_final$death)
boost.accuracy
#confusion matrix
table(boost.pred, train_final$death)
#auc
pROC::auc(roc(labels,boost.probs))
#make predictions on test set using full boost model
boost.test.probs = predict(boost.test, test_df[,c(-1)], n.trees = 1000, type='response')

#make final df with patient id and prediction
final_preds = data.frame(patient_id = test_df[,c(1)], boost.pred = boost.test.probs)
#write to csv
write.csv(final_preds,'/Users/Brian/Dropbox/UTHealth/PH 1976/predicts_mortality/final_preds2.csv', row.names = FALSE)
```



